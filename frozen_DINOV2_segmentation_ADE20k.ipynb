{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n2SaJAfe4qF3"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import shutil\n",
        "import argparse\n",
        "import zipfile\n",
        "import hashlib\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import hashlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from transformers import AutoImageProcessor\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, loss, epoch, checkpoint_path=\"model_checkpoint.pth\"):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "# Load checkpoint function\n",
        "def load_checkpoint(model, optimizer, checkpoint_path=\"model_checkpoint.pth\"):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Checkpoint loaded. Resuming from epoch {start_epoch} with loss {loss:.4f}.\")\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "TZB_ImLptkfd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check SHA1 hash to ensure file integrity\n",
        "def check_sha1(filename, sha1_hash):\n",
        "    sha1 = hashlib.sha1()\n",
        "    with open(filename, 'rb') as f:\n",
        "        while True:\n",
        "            data = f.read(1048576)\n",
        "            if not data:\n",
        "                break\n",
        "            sha1.update(data)\n",
        "    return sha1.hexdigest()[:len(sha1_hash)] == sha1_hash\n",
        "\n",
        "# Download the dataset from the URL\n",
        "def download(url, path=None, overwrite=False, sha1_hash=None):\n",
        "    if path is None:\n",
        "        fname = url.split('/')[-1]\n",
        "    else:\n",
        "        fname = os.path.join(path, url.split('/')[-1]) if os.path.isdir(path) else path\n",
        "\n",
        "    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n",
        "        os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "        print(f'Downloading {fname} from {url}...')\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(fname, 'wb') as f:\n",
        "                for chunk in tqdm(r.iter_content(chunk_size=1024), unit='KB', dynamic_ncols=True):\n",
        "                    f.write(chunk)\n",
        "        if sha1_hash and not check_sha1(fname, sha1_hash):\n",
        "            raise ValueError(f'File {fname} does not match hash {sha1_hash}')\n",
        "    return fname\n",
        "\n",
        "# Download ADE20K dataset\n",
        "def download_ade(path, overwrite=False):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "    _AUG_DOWNLOAD_URLS = [\n",
        "        ('http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip', '219e1696abb36c8ba3a3afe7fb2f4b4606a897c7'),\n",
        "    ]\n",
        "    download_dir = os.path.join(path, 'downloads')\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    for url, checksum in _AUG_DOWNLOAD_URLS:\n",
        "        filename = download(url, path=download_dir, overwrite=overwrite, sha1_hash=checksum)\n",
        "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(path=path)\n",
        "\n",
        "# Dataset paths\n",
        "root = \"./\"\n",
        "dataset_path = os.path.join(root, \"ADEChallengeData2016/images/\")\n",
        "training_data = \"training/\"\n",
        "val_data = \"validation/\"\n",
        "\n",
        "download_ade(root, overwrite=False)\n",
        "\n",
        "N_CLASSES = 151\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Load image and mask paths\n",
        "train_images = sorted(glob(os.path.join(dataset_path, training_data, \"*.jpg\")))\n",
        "train_masks = sorted(glob(os.path.join(root, \"ADEChallengeData2016/annotations/training/*.png\")))\n",
        "val_images = sorted(glob(os.path.join(dataset_path, val_data, \"*.jpg\")))\n",
        "val_masks = sorted(glob(os.path.join(root, \"ADEChallengeData2016/annotations/validation/*.png\")))\n"
      ],
      "metadata": {
        "id": "aqXUJZPVXHg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class ADE20KDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, img_size=224):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),\n",
        "            transforms.Lambda(lambda x: torch.from_numpy(np.array(x, dtype=np.int64))),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.mask_paths[idx]).convert(\"P\")\n",
        "        image = self.image_transform(image)\n",
        "        mask = self.mask_transform(mask).squeeze(0).long()\n",
        "        return image, mask\n",
        "\n",
        "# Segmentation model\n",
        "class DINOv2Segmentation(torch.nn.Module):\n",
        "    def __init__(self, base_model, num_classes, embed_dim=384, freeze_backbone=True):\n",
        "        super(DINOv2Segmentation, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(embed_dim, 256, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv2d(128, num_classes, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        # Freeze backbone if specified\n",
        "        if freeze_backbone:\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.base_model.get_intermediate_layers(x, n=1)[0]\n",
        "        B, N, C = features.size()  # Batch size, Tokens, Channels\n",
        "        H = W = int(N ** 0.5)  # Square spatial resolution\n",
        "        features = features.permute(0, 2, 1).view(B, C, H, W)  # Reshape to (B, C, H, W)\n",
        "        return self.decoder(features)\n",
        "\n",
        "# Training setup\n",
        "train_dataset = ADE20KDataset(train_images, train_masks, img_size=IMG_SIZE)\n",
        "val_dataset = ADE20KDataset(val_images, val_masks, img_size=IMG_SIZE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "\n",
        "# Initialize the model with frozen backbone\n",
        "model = DINOv2Segmentation(dinov2_vits14, N_CLASSES, embed_dim=384, freeze_backbone=True).to(device)\n",
        "\n",
        "# Only include trainable parameters in the optimizer\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "epochs = 150\n",
        "\n",
        "checkpoint_path = \"frozen_model_checkpoint.pth\"\n",
        "\n",
        "# Attempt to load checkpoint\n",
        "resume_training = True\n",
        "if resume_training:\n",
        "    try:\n",
        "        start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No checkpoint found. Starting training from scratch.\")\n",
        "        start_epoch = 0\n",
        "else:\n",
        "    start_epoch = 1\n",
        "\n",
        "print(f\"Starting training from epoch {start_epoch}.\")\n",
        "# Training loop with tqdm progress bar\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    with tqdm(train_loader, unit='batch', desc=f'Epoch {epoch+1}/{epochs}') as tepoch:\n",
        "        for images, masks in tepoch:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # Shape: [B, N_CLASSES, H, W]\n",
        "\n",
        "            # Upsample the outputs to match the size of the masks\n",
        "            outputs_upsampled = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs_upsampled, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update tqdm description with loss information\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    save_checkpoint(model, optimizer, loss.item(), epoch, checkpoint_path)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Training loop with limited batches\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    max_batches = 100  # Number of batches to test\n",
        "    batch_count = 0\n",
        "    with tqdm(train_loader, unit='batch', desc=f'Epoch {epoch+1}/10') as tepoch:\n",
        "        for images, masks in tepoch:\n",
        "            if batch_count >= max_batches:  # Stop after max_batches\n",
        "                break\n",
        "\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # Shape: [B, N_CLASSES, H, W]\n",
        "\n",
        "            # Upsample the outputs to match the size of the masks\n",
        "            outputs_upsampled = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs_upsampled, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update tqdm description with loss information\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "            batch_count += 1  # Increment batch counter\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/10, Loss: {loss.item()}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cRjzjAfN6ZPW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function\n",
        "def visualize_predictions(model, loader, device):\n",
        "    model.eval()\n",
        "    images, masks = next(iter(loader))\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        outputs_upsampled = F.interpolate(outputs, size=masks.shape[1:], mode=\"bilinear\", align_corners=False)\n",
        "        predictions = torch.argmax(outputs_upsampled, dim=1).cpu().numpy()\n",
        "\n",
        "    # Visualize first batch\n",
        "    for i in range(min(4, images.size(0))):\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # De-normalize\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Input Image\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(masks[i].cpu(), cmap=\"viridis\")\n",
        "        plt.title(\"Ground Truth Mask\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(predictions[i], cmap=\"viridis\")\n",
        "        plt.title(\"Predicted Mask\")\n",
        "        plt.show()\n",
        "\n",
        "# Visualize some predictions\n",
        "visualize_predictions(model, val_loader, device)"
      ],
      "metadata": {
        "id": "zCZo8MR_h54q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate IoU for each class\n",
        "def compute_iou(pred, target, n_classes=N_CLASSES):\n",
        "    iou_list = []\n",
        "    for i in range(n_classes):\n",
        "        # Create a binary mask for class i\n",
        "        pred_i = (pred == i).float()\n",
        "        target_i = (target == i).float()\n",
        "\n",
        "        intersection = torch.sum(pred_i * target_i)\n",
        "        union = torch.sum(pred_i) + torch.sum(target_i) - intersection\n",
        "\n",
        "        iou = intersection / union if union != 0 else torch.tensor(0.0)\n",
        "        iou_list.append(iou)\n",
        "    return iou_list\n",
        "\n",
        "# Function to display images and masks\n",
        "def visualize_predictions(images, masks, preds, idx=0):\n",
        "    # Image\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(images[idx].permute(1, 2, 0).cpu().numpy())\n",
        "    axes[0].set_title(\"Image\")\n",
        "\n",
        "    # Ground Truth\n",
        "    axes[1].imshow(masks[idx].cpu().numpy())\n",
        "    axes[1].set_title(\"Ground Truth\")\n",
        "\n",
        "    # Prediction\n",
        "    axes[2].imshow(preds[idx].cpu().numpy())\n",
        "    axes[2].set_title(\"Prediction\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_iou = []  # To store IoU for each class across all batches\n",
        "\n",
        "    # No gradient calculation needed during inference\n",
        "    with torch.no_grad():\n",
        "        # Loop through the test dataset\n",
        "        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Forward pass: Get the outputs of the model\n",
        "            outputs = model(images)  # Shape: [B, N_CLASSES, 16, 16]\n",
        "\n",
        "            # Upsample the outputs to match the mask shape\n",
        "            outputs_upsampled = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Get the predictions (class with highest probability)\n",
        "            preds = torch.argmax(outputs_upsampled, dim=1)  # Shape: [B, H, W]\n",
        "\n",
        "            # Compute IoU for each batch\n",
        "            iou_list = compute_iou(preds, masks)\n",
        "            all_iou.append(iou_list)\n",
        "\n",
        "            # Optionally visualize the predictions for the first image\n",
        "            visualize_predictions(images, masks, preds, idx=0)\n",
        "\n",
        "    # Calculate average IoU across all batches\n",
        "    avg_iou = np.mean(all_iou, axis=0)\n",
        "    print(f\"Average IoU per class: {avg_iou}\")\n",
        "\n",
        "    return avg_iou\n",
        "\n",
        "test_dataset = ADE20KDataset(val_images, val_masks, img_size=IMG_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "# Run the test\n",
        "avg_iou = test_model(model, test_loader)"
      ],
      "metadata": {
        "id": "sgmQBwfghgct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_miou(predictions, ground_truths, num_classes):\n",
        "    \"\"\"\n",
        "    Computes the mean Intersection over Union (mIoU) with tqdm progress bar.\n",
        "\n",
        "    Args:\n",
        "        predictions (Tensor): Predicted segmentation maps, shape (B, H, W).\n",
        "        ground_truths (Tensor): Ground truth segmentation maps, shape (B, H, W).\n",
        "        num_classes (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        float: mIoU score.\n",
        "    \"\"\"\n",
        "    ious = []\n",
        "    for cls in tqdm(range(num_classes), desc=\"Calculating IoU for classes\", unit=\"class\"):\n",
        "        intersection = ((predictions == cls) & (ground_truths == cls)).sum().item()\n",
        "        union = ((predictions == cls) | (ground_truths == cls)).sum().item()\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # Ignore this class in mIoU computation\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "    miou = np.nanmean(ious)  # Average over all valid classes\n",
        "    return miou\n"
      ],
      "metadata": {
        "id": "BRJEd4DOv9KX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader, device, num_classes):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Evaluating\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            outputs_upsampled = F.interpolate(outputs, size=masks.shape[1:], mode=\"bilinear\", align_corners=False)\n",
        "            predictions = torch.argmax(outputs_upsampled, dim=1)  # Shape: (B, H, W)\n",
        "\n",
        "            all_predictions.append(predictions.cpu())\n",
        "            all_ground_truths.append(masks.cpu())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_predictions = torch.cat(all_predictions, dim=0)\n",
        "    all_ground_truths = torch.cat(all_ground_truths, dim=0)\n",
        "\n",
        "    # Compute mIoU\n",
        "    miou = compute_miou(all_predictions, all_ground_truths, num_classes=num_classes)\n",
        "    return miou"
      ],
      "metadata": {
        "id": "pYvSbNXJwIRo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_multiscale(model, loader, device, num_classes, scales=[0.5, 1.0, 1.5, 2.0]):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Evaluating (Multiscale)\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            batch_size, _, orig_height, orig_width = images.size()\n",
        "\n",
        "            # Multiscale predictions\n",
        "            scale_predictions = []\n",
        "            for scale in scales:\n",
        "                scaled_images = F.interpolate(images, scale_factor=scale, mode='bilinear', align_corners=False)\n",
        "                outputs = model(scaled_images)\n",
        "                outputs_upsampled = F.interpolate(outputs, size=(orig_height, orig_width), mode=\"bilinear\", align_corners=False)\n",
        "                scale_predictions.append(outputs_upsampled)\n",
        "\n",
        "            # Average predictions across scales\n",
        "            averaged_outputs = torch.mean(torch.stack(scale_predictions, dim=0), dim=0)\n",
        "            predictions = torch.argmax(averaged_outputs, dim=1)  # Shape: (B, H, W)\n",
        "\n",
        "            all_predictions.append(predictions.cpu())\n",
        "            all_ground_truths.append(masks.cpu())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_predictions = torch.cat(all_predictions, dim=0)\n",
        "    all_ground_truths = torch.cat(all_ground_truths, dim=0)\n",
        "\n",
        "    # Compute mIoU\n",
        "    miou = compute_miou(all_predictions, all_ground_truths, num_classes=num_classes)\n",
        "    return miou\n"
      ],
      "metadata": {
        "id": "6-_sp76hwQ-o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear evaluation (single scale)\n",
        "test_dataset = ADE20KDataset(val_images, val_masks, img_size=IMG_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "miou_test = evaluate_model(model, test_loader, device, num_classes=N_CLASSES)\n",
        "print(f\"Mean IoU on ADE20K test set: {miou_test:.4f}\")\n",
        "\n",
        "# Multiscale evaluation on test set\n",
        "miou_multiscale_test = evaluate_model_multiscale(model, test_loader, device, num_classes=N_CLASSES, scales=[0.5, 1.0, 1.5, 2.0])\n",
        "print(f\"Multiscale Mean IoU on ADE20K test set: {miou_multiscale_test:.4f}\")"
      ],
      "metadata": {
        "id": "YbG4qoDvwTAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}